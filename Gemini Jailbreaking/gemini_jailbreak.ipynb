{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging .csv Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Pandas directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 2 csv files (The Template and Questions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPTFuzzer.csv file\n",
    "sentences = pd.read_csv('GPTFuzzer.csv')\n",
    "\n",
    "# Load the question_list.csv file\n",
    "replacements = pd.read_csv('question_list.csv')\n",
    "\n",
    "# Create a new dataframe to store the output\n",
    "output = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPTFuzzer.csv template has \\[INSERT PROMPT HERE] inside. Replace the sentence with the contents in \"question_list.csv\" and save it to all_questions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each sentence\n",
    "for index, sentence in sentences.iterrows():\n",
    "\n",
    "    # Create a new row for the output dataframe\n",
    "    new_row = sentence.copy()\n",
    "\n",
    "    # Loop through each replacement\n",
    "    for replacement in replacements.iloc[:, 1:].iloc[:, 0]:  \n",
    "\n",
    "        # Replace the [INSERT PROMPT HERE] with the replacement\n",
    "        new_row['text'] = sentence['text'].replace('[INSERT PROMPT HERE]', replacement)\n",
    "        \n",
    "        # Add the new row to the output dataframe\n",
    "        output = pd.concat([output, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Create a new 'id' column with sequential values\n",
    "output['id'] = range(len(output))\n",
    "\n",
    "# Save the output to a new CSV file\n",
    "output.to_csv('all_questions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the new csv files into parts of 10.\n",
    "\n",
    "This is useful when a timeout has occured, and you only need to modify the code a bit and continue from the last timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the all_questions.csv file\n",
    "df = pd.read_csv('all_questions.csv')\n",
    "\n",
    "# Define how many lists in each split\n",
    "split_size = 10\n",
    "\n",
    "# Create a list to store the splitted DataFrames\n",
    "split_dfs = []\n",
    "\n",
    "# Split the DataFrame\n",
    "for i in range(0, len(df), split_size):\n",
    "    split = df.iloc[i:i+split_size]\n",
    "    split_dfs.append(split)\n",
    "\n",
    "# Save each split to a new csv file (This will be named q{1, 2,...}.csv)\n",
    "for i, split in enumerate(split_dfs):\n",
    "    split.to_csv(f'q{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Gemini AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Google AI SDK in cmd:\n",
    "\n",
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and use the API key to be able to use Gemini\n",
    "\n",
    "Getting the API key:\n",
    "\n",
    "https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fmakersuite.google.com%2Fapp%2Fapikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=\"[INSERT THE API KEY HERE]\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this one, I am choosing gemini-1.5-flash as the Model, but when using the Paid version, I recommend to use gemini-1.5-pro instead.\n",
    "\n",
    "For a brief reasoning:\n",
    "\n",
    "For free users, gemini-1.5-flash gives 15 Requests per Minute, and a maximum of 1500 Requests per Day.\n",
    "\n",
    "And the free users using gemini-1.5-pro gets only 2 Requests per Minute.\n",
    "\n",
    "Paid users have very high limits, which makes it more efficient doing these stuffs.\n",
    "\n",
    "source: https://ai.google.dev/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the **Free** version of Gemini, the prompt input is limited to 15 requests per minutes, and it has a limit of 1500 requests per day. To bypass this, I use a 60-second timer for each 1 csv file that has been answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the index number\n",
    "num = 1\n",
    "\n",
    "for i in range(769):\n",
    "\n",
    "    #Load all the q.csv files\n",
    "    df = pd.read_csv(f'q{1+i}.csv')\n",
    "\n",
    "    #Loop the lists inside the csv file\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = row.iloc[1]\n",
    "\n",
    "        #Generate the response\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        if response.parts:\n",
    "            answer = response.parts[0].text\n",
    "        else:\n",
    "            answer = \"\\\"No response generated\\\"\"\n",
    "        \n",
    "        #Insert the output to a new \"answer_gemini.csv\" file\n",
    "        answer_df = pd.DataFrame({'id': [num], 'answer': [answer]})\n",
    "        answer_df.to_csv(f'answer_gemini.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        #Add the next index number\n",
    "        num += 1\n",
    "    \n",
    "    # Wait for 60 seconds\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the **Paid** version, it doesnt need any timer to delay, so it is recommended to run this code instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the index number\n",
    "num = 1\n",
    "\n",
    "for i in range(769):\n",
    "\n",
    "    #Load all the q.csv files\n",
    "    df = pd.read_csv(f'q{1+i}.csv')\n",
    "\n",
    "    #Loop the lists inside the csv file\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = row.iloc[1]\n",
    "\n",
    "        #Generate the response\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        if response.parts:\n",
    "            answer = response.parts[0].text\n",
    "        else:\n",
    "            answer = \"\\\"No response generated\\\"\"\n",
    "        \n",
    "        #Insert the output to a new \"answer_gemini.csv\" file\n",
    "        answer_df = pd.DataFrame({'id': [num], 'answer': [answer]})\n",
    "        answer_df.to_csv(f'answer_gemini.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        #Add the next index number\n",
    "        num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
