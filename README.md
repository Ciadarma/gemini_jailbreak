# gemini_jailbreak
Jailbreaking Gemini AI using Prompts

This is an University Project to determine if Google's Generative AI, Gemini, is vulnerable to prompts that deviates the normal response by using prompts that has been crafted in a way to make the response deviated.

Templates, Questions, and References are from GPTFuzzer: https://github.com/sherdencooper/GPTFuzz
